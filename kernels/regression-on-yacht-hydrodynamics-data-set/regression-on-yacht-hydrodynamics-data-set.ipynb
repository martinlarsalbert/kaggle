{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression on yacht hydrodynamics data set\n\n![](https://www.blur.se/images/df4.jpg)\nI have previsouly fitted a polynomial to this dataset which I wrote about on [my blog](https://martinlarsalbert.github.io/blog/machine%20learning/regression/2020/08/17/regression_yacht_hydrodynamics_data_set.html).\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport altair as alt\nfrom io import StringIO\nimport re\nimport urllib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load dataset\n[Dataset](https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics) from from Technical University of Delft."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#collapse\ncolumns = [\n'lcg',   \n'cp',    \n'volume',\n'b/d',   \n'l/b',   \n'fn',    \n'r', \n]\n\ndata_url = r'http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'\nwith urllib.request.urlopen(data_url) as file:\n    s_raw=file.read().decode(\"utf-8\")\n    \n# remove some dirt:\nregexp = re.compile(r' \\n', flags=re.DOTALL)\ns1 = regexp.sub('\\n', s_raw)\n\nregexp = re.compile(r' +', flags=re.DOTALL)\ns2 = regexp.sub(' ', s1)\ns2[0:200]\ns=s2\n\ndata = StringIO(s)\ndata = pd.read_csv(data, sep=' ', encoding='utf-8', names=columns)\n\nfeatures = list(set(columns)-set(['r']))\nlabel = 'r'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX=data[features].copy()\ny=data[label].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nstandard_scaler = StandardScaler()\nX_train_scaled = standard_scaler.fit_transform(X_train)\nX_test_scaled = standard_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(X_train_scaled, y_train)\nimportances = pd.DataFrame(data={\n    'Attribute': X_train.columns,\n    'Importance': model.feature_importances_\n})\nimportances = importances.sort_values(by='Importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obtain importances from a tree-based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')\nplt.title('Feature importances obtained from coefficients', size=20)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass FeatureSelector( BaseEstimator):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self._feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[['fn']] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fn_selector = FeatureSelector(feature_names=[['fn']])\nfn_selector.fit(X_train)\nfn_selector.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\n\npolynomial_features = PolynomialFeatures(degree=2)\nlinear_regression = LinearRegression()\n\nsteps = [\n    ('fn_selector', fn_selector),\n    ('polynomial_features', polynomial_features),\n    ('linear_regression', linear_regression),\n]\n\nmodel_benchmark = Pipeline(steps=steps)\n\n\n# define the grid\ngrid = dict()\ngrid['polynomial_features__degree'] = [i for i in range(1, 6)]\n\n\n# define the grid search\ncv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\nsearch = GridSearchCV(estimator=model_benchmark, param_grid=grid, scoring='r2', n_jobs=-1, cv=cv)\n\n# perform the search\n\nsearch_result = search.fit(X_train, y_train)\n\nmodel_benchmark = search_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_benchmark_ = Pipeline(steps=[\n    ('polynomial_features', PolynomialFeatures(degree=4)),\n    ('linear_regression', LinearRegression()),\n])\nmodel_benchmark_.fit(X_train[['fn']], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_result.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_benchmark.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_benchmark_.score(X_test[['fn']], y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_benchmark.predict(X_test)\nfig,ax=plt.subplots()\nx = X_test['fn']\nax.plot(x,y_test, '+', label='test')\nax.plot(x,y_pred, '.', label='prediction')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = model_benchmark_['linear_regression']\nl.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so a 4th degree polynomial on froude number **fn** can give a very good prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}